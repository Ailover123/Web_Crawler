#To remove needless noise from baseline data.
# Input: parsed HTML structure
# Process:
#   - normalize tags
#   - normalize attributes
#   - clean whitespace
# Output: stable string

def normalize_html(soup):
  for tag in soup.find_all(True):
      tag.name = tag.name.lower()
# HTML tag names are case-insensitive; normalize to lowercase
      if tag.attrs:
         #Normalise class lists
         if "class" in tag.attrs:
          tag.attrs["class"] = sorted(tag.attrs["class"])
          #Sort attributes
         tag.attrs = dict(sorted(tag.attrs.items()))
# Sort attributes to ensure consistent ordering so that string comparison are reliable
  return str(soup)


def normalize_url(url):
    """
    Normalize URLs by removing trailing slash from path, except for root.
    Do NOT normalize query params, fragments, or CMS paths.
    """
    from urllib.parse import urlparse, urlunparse
    try:
        p = urlparse(url)
        # Only modify path: remove trailing / if not root
        path = p.path
        if path.endswith('/') and path != '/':
            path = path.rstrip('/')
        normalized = urlunparse((p.scheme, p.netloc, path, p.params, p.query, p.fragment))
        return normalized
    except Exception:
        return url


def strip_trivial_comments(html_text):
  """Remove HTML comments that are known to be cache/footprint noise.

  This strips comments like LiteSpeed Cache footers and timestamps which
  frequently change between requests but do not indicate content defacement.
  The removal is done by regex and returns the cleaned HTML string.
  """
  import re
  if not html_text:
    return html_text
  patterns = [
    r'<!--\s*Page supported by LiteSpeed Cache.*?-->',
    r'<!--\s*LSCACHE:.*?-->',
    r'<!--\s*Cached page generated by.*?-->',
    r'<!--\s*Cache created at.*?-->',
  ]
  out = html_text
  for pat in patterns:
    try:
      out = re.sub(pat, '', out, flags=re.I | re.S)
    except Exception:
      continue
  return out


def semantic_normalize_html(html_text):
  """Perform semantic normalization on HTML text before hashing.

  - Collapse whitespace
  - Normalize punctuation spacing
  - Canonicalize short comma-separated lists by sorting tokens (helps
    with location strings like "Dubai, Middle East, UAE" vs "Middle East, UAE, Dubai").
  """
  from bs4 import BeautifulSoup
  import re
  if not html_text:
    return html_text
  try:
    soup = BeautifulSoup(html_text, 'html.parser')
    # Iterate over text nodes and normalize
    for element in soup.find_all(text=True):
      # Skip scripts and styles
      if element.parent.name in ['script', 'style', 'noscript']:
        continue
      text = str(element)
      # Collapse whitespace
      t = re.sub(r'\s+', ' ', text).strip()
      # Normalize punctuation spacing (remove space before comma, ensure single space after)
      t = re.sub(r'\s*,\s*', ', ', t)
      # Canonicalize short comma-separated lists
      if ',' in t:
        parts = [p.strip() for p in t.split(',') if p.strip()]
        # Heuristic: only canonicalize when parts are short (<=5 words each) and few parts
        if 1 < len(parts) <= 6 and all(len(p.split()) <= 5 for p in parts):
          # Sort tokens case-insensitively to create canonical order
          try:
            sorted_parts = sorted(parts, key=lambda s: s.lower())
            t = ', '.join(sorted_parts)
          except Exception:
            pass
      # Replace text node only if changed
      if t != text:
        try:
          element.replace_with(t)
        except Exception:
          pass
    return str(soup)
  except Exception:
    # If parsing fails, fall back to whitespace/punctuation normalization on raw text
    import re
    out = re.sub(r'\s+', ' ', html_text).strip()
    out = re.sub(r'\s*,\s*', ', ', out)
    return out


def dom_structure_fingerprint(html_text):
  """Generate a lightweight DOM structure fingerprint: set of tag paths and counts.

  This is used to detect node additions/removals without performing expensive diffing.
  """
  from bs4 import BeautifulSoup
  if not html_text:
    return None
  try:
    soup = BeautifulSoup(html_text, 'html.parser')
    paths = []
    def walk(node, path=''):
      for child in node.children:
        if getattr(child, 'name', None):
          tag = child.name.lower()
          newpath = f"{path}/{tag}"
          paths.append(newpath)
          walk(child, newpath)
    walk(soup, '')
    # Return a sorted tuple so comparisons are deterministic
    return tuple(sorted(paths))
  except Exception:
    return None

